{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse as ap\n",
    "import cv2\n",
    "import imutils2 as imutils \n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from scipy.cluster.vq import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train\"\n",
    "training_names = os.listdir(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small', 'big']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the path to the images and save them in a list\n",
    "# image_paths and the corresponding label in image_paths\n",
    "image_paths = []\n",
    "image_classes = []\n",
    "class_id = 0\n",
    "for training_name in training_names:\n",
    "    dir = os.path.join(train_path, training_name)\n",
    "    class_path = imutils.imlist(dir)\n",
    "    #class_path=list(paths.list_images(dir))\n",
    "    #print class_path\n",
    "    image_paths+=class_path\n",
    "    image_classes+=[class_id]*len(class_path)\n",
    "    class_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature extraction and keypoint detector objects\n",
    "orb = cv2.ORB_create()\n",
    "surf=cv2.xfeatures2d.SURF_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List where all the descriptors are stored\n",
    "des_list_orb = []\n",
    "des_list_surf=[]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    im = cv2.imread(image_path)\n",
    "    #kpts = fea_det.detect(im)\n",
    "    #kpts, des = des_ext.compute(im, kpts)\n",
    "    kpts = orb.detect(im,None)\n",
    "    kpts, des = orb.compute(im, kpts)\n",
    "    skp,sdes=surf.detectAndCompute(im,None)\n",
    "    des_list_orb.append((image_path, des.astype(float)))\n",
    "    des_list_surf.append((image_path, sdes)) \n",
    "    #des_list.append((image_path, sdes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the descriptors vertically in a numpy array\n",
    "descriptors1 = des_list_orb[0][1]\n",
    "for image_path, descriptor in des_list_orb[1:]:\n",
    "    descriptors1 = np.vstack((descriptors1, descriptor))  \n",
    "    \n",
    "descriptors2 = des_list_surf[0][1]\n",
    "for image_path, descriptor in des_list_surf[1:]:\n",
    "    descriptors2 = np.vstack((descriptors2, descriptor)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-means clustering\n",
    "k1 = 120\n",
    "voc1, variance1 = kmeans(descriptors1, k1, 1) \n",
    "\n",
    "k2 = 120\n",
    "voc2, variance2 = kmeans(descriptors2, k2, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the histogram of features\n",
    "im_features1 = np.zeros((len(image_paths), k1), \"float32\")\n",
    "for i in xrange(len(image_paths)):\n",
    "    words, distance = vq(des_list_orb[i][1],voc1)\n",
    "    for w in words:\n",
    "        im_features1[i][w] += 1\n",
    "        \n",
    "im_features2 = np.zeros((len(image_paths), k2), \"float32\")\n",
    "for i in xrange(len(image_paths)):\n",
    "    words, distance = vq(des_list_surf[i][1],voc2)\n",
    "    for w in words:\n",
    "        im_features2[i][w] += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_features=np.concatenate((im_features1,im_features2),axis=1)\n",
    "#im_features=im_features2\n",
    "\n",
    "nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "stdSlr = StandardScaler().fit(im_features)\n",
    "im_features = stdSlr.transform(im_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Perform Tf-Idf vectorization\n",
    "nbr_occurences = np.sum( (im_features1 > 0) * 1, axis = 0)\n",
    "idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "\n",
    "nbr_occurences = np.sum( (im_features2 > 0) * 1, axis = 0)\n",
    "idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Scaling the words\n",
    "\n",
    "stdSlr = StandardScaler().fit(im_features1)\n",
    "im_features1 = stdSlr.transform(im_features1)\n",
    "\n",
    "stdSlr = StandardScaler().fit(im_features2)\n",
    "im_features2 = stdSlr.transform(im_features2)\n",
    "\n",
    "\n",
    "im_features=np.concatenate((im_features1,im_features2),axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=150)\n",
    "pca.fit(im_features)\n",
    "pca_f=pca.transform(im_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis as FA\n",
    "fa = FA(n_components=150)\n",
    "fa.fit(im_features)\n",
    "fa_f=fa.transform(im_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='rbf', C=4)\n",
    "#clf = RandomForestClassifier(max_depth=20)\n",
    "scores = cross_val_score(clf, im_features, np.array(image_classes), cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump((clf, training_names, stdSlr, [k1,k2], [voc1,voc2]), \"bof.pkl\", compress=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
